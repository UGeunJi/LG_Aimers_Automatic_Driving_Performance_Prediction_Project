{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a9d65b-2090-40f3-b991-6718361c57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor, RegressorChain\n",
    "from sklearn import metrics\n",
    "\n",
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(14): # ignore 'ID'\n",
    "        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:15])\n",
    "    return score, all_nrmse\n",
    "\n",
    "def train_and_predict_single(df, col):\n",
    "    train_df, valid_df = train_test_split(df, train_size=0.9)\n",
    "\n",
    "    train_x = train_df.filter(regex='X') # Input : X Featrue\n",
    "    train_y = train_df.filter(regex='Y')[col] # Output : Y Feature\n",
    "\n",
    "    valid_x = valid_df.filter(regex='X') # Input : X Featrue\n",
    "    valid_y = valid_df.filter(regex='Y')[col] # Output : Y Feature\n",
    "\n",
    "    LR = LinearRegression().fit(train_x, train_y)\n",
    "    valid_preds = LR.predict(valid_x)\n",
    "    \n",
    "    rmse = metrics.mean_squared_error(valid_y, valid_preds, squared=False)\n",
    "    nrmse = rmse/np.mean(np.abs(valid_y))\n",
    "\n",
    "    return nrmse\n",
    "    \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ddbc1db-e01b-4e9b-a4bb-435458d64a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.005765865905685,\n",
       " [0.263815936718668,\n",
       "  0.3651145844355974,\n",
       "  0.3597224977179273,\n",
       "  0.19118359676732682,\n",
       "  0.07961938614664572,\n",
       "  0.11404120552786717,\n",
       "  0.1298247508646181,\n",
       "  0.024620529495198832,\n",
       "  0.024431407359814288,\n",
       "  0.04008515923409129,\n",
       "  0.03399353979572566,\n",
       "  0.024625581962635024,\n",
       "  0.02449237270957379,\n",
       "  0.02460681963522579])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(df, train_size=0.8)\n",
    "\n",
    "train_x = train_df.filter(regex='X') # Input : X Featrue\n",
    "train_y = train_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "valid_x = valid_df.filter(regex='X') # Input : X Featrue\n",
    "valid_y = valid_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "LR = MultiOutputRegressor(LinearRegression()).fit(train_x, train_y)\n",
    "\n",
    "valid_preds = LR.predict(valid_x)\n",
    "total, scores = lg_nrmse(valid_y.values, valid_preds)\n",
    "\n",
    "baseline_score = {col : score for col, score in zip(train_y.columns, scores)}\n",
    "lg_nrmse(valid_y.values, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3583aa7c-f384-4d2b-a95a-9d933edcb596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9713591607787473,\n",
       " [0.2586467056955648,\n",
       "  0.3578216294713198,\n",
       "  0.35072195303057413,\n",
       "  0.19072310009920118,\n",
       "  0.07970801185696683,\n",
       "  0.11190386680952791,\n",
       "  0.128193778469416,\n",
       "  0.024103501150637915,\n",
       "  0.02395859245826794,\n",
       "  0.03932740124960672,\n",
       "  0.033658520683682955,\n",
       "  0.024049892682118386,\n",
       "  0.024118049190284405,\n",
       "  0.024059648614936528])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(df, train_size=0.8)\n",
    "\n",
    "train_x = train_df.filter(regex='X') # Input : X Featrue\n",
    "train_y = train_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "valid_x = valid_df.filter(regex='X') # Input : X Featrue\n",
    "valid_y = valid_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "LR = LinearRegression()#.fit(train_x, train_y)\n",
    "chain =  RegressorChain(base_estimator=LR, order=[13, 12, 11, 10, 9, 8, 7, 4, 5, 6, 3, 0, 1, 2], random_state=42).fit(train_x, train_y)\n",
    "\n",
    "valid_preds = chain.predict(valid_x)\n",
    "lg_nrmse(valid_y.values, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdbf4c-c77f-4d5f-be21-dc5022208ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92613055-ecfe-4a62-a06e-1d77b4249535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7210597718341976,\n",
       " [0.22534964593805876,\n",
       "  0.3249878199847411,\n",
       "  0.3165278548325993,\n",
       "  0.17921083909473023,\n",
       "  0.07657903440332359,\n",
       "  0.04340901003781572,\n",
       "  0.1145297878946722,\n",
       "  0.02297454402172718,\n",
       "  0.02279723936630012,\n",
       "  0.03378939841731452,\n",
       "  0.03121745292937744,\n",
       "  0.022958576064740995,\n",
       "  0.022972596201979945,\n",
       "  0.023042265405282838])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('normal_train.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(df, train_size=0.8)\n",
    "\n",
    "train_x = train_df.filter(regex='X') # Input : X Featrue\n",
    "train_y = train_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "valid_x = valid_df.filter(regex='X') # Input : X Featrue\n",
    "valid_y = valid_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "LR = MultiOutputRegressor(LinearRegression()).fit(train_x, train_y)\n",
    "\n",
    "valid_preds = LR.predict(valid_x)\n",
    "lg_nrmse(valid_y.values, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e4705b-4369-452c-8333-9f328d65246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0328524381394826,\n",
       " [0.3985060812794724,\n",
       "  0.47130181337133054,\n",
       "  0.461574012853423,\n",
       "  0.2825125857430445,\n",
       "  0.11007071543915137,\n",
       "  0.32417660881649274,\n",
       "  0.23447542324489848,\n",
       "  0.0333049256949685,\n",
       "  0.03323466315232737,\n",
       "  0.06912069583200375,\n",
       "  0.05210349949119037,\n",
       "  0.033170928650362294,\n",
       "  0.033012057311208695,\n",
       "  0.033103993971052216])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('abnormal_train.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(df, train_size=0.8)\n",
    "\n",
    "train_x = train_df.filter(regex='X') # Input : X Featrue\n",
    "train_y = train_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "valid_x = valid_df.filter(regex='X') # Input : X Featrue\n",
    "valid_y = valid_df.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "LR = MultiOutputRegressor(LinearRegression()).fit(train_x, train_y)\n",
    "\n",
    "valid_preds = LR.predict(valid_x)\n",
    "lg_nrmse(valid_y.values, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025d9239-0c04-4ca1-af9f-2da65b849e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.333359</td>\n",
       "      <td>1.034397</td>\n",
       "      <td>0.988477</td>\n",
       "      <td>13.722972</td>\n",
       "      <td>31.342709</td>\n",
       "      <td>16.643526</td>\n",
       "      <td>3.148128</td>\n",
       "      <td>-26.269909</td>\n",
       "      <td>-26.283590</td>\n",
       "      <td>-22.352684</td>\n",
       "      <td>24.357592</td>\n",
       "      <td>-26.212676</td>\n",
       "      <td>-26.208827</td>\n",
       "      <td>-26.220969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305719</td>\n",
       "      <td>0.344410</td>\n",
       "      <td>0.315419</td>\n",
       "      <td>2.496548</td>\n",
       "      <td>2.409967</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.361532</td>\n",
       "      <td>0.623804</td>\n",
       "      <td>0.617551</td>\n",
       "      <td>0.788934</td>\n",
       "      <td>0.764020</td>\n",
       "      <td>0.619848</td>\n",
       "      <td>0.618885</td>\n",
       "      <td>0.619733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7.002000</td>\n",
       "      <td>22.033000</td>\n",
       "      <td>12.530000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>-29.129000</td>\n",
       "      <td>-29.010000</td>\n",
       "      <td>-26.475000</td>\n",
       "      <td>20.175000</td>\n",
       "      <td>-29.082000</td>\n",
       "      <td>-29.102000</td>\n",
       "      <td>-29.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.131000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>11.986250</td>\n",
       "      <td>29.880250</td>\n",
       "      <td>16.199000</td>\n",
       "      <td>2.872000</td>\n",
       "      <td>-26.659000</td>\n",
       "      <td>-26.670750</td>\n",
       "      <td>-22.823000</td>\n",
       "      <td>23.883000</td>\n",
       "      <td>-26.601000</td>\n",
       "      <td>-26.593000</td>\n",
       "      <td>-26.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.340000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.983000</td>\n",
       "      <td>13.909000</td>\n",
       "      <td>31.718000</td>\n",
       "      <td>16.706000</td>\n",
       "      <td>3.121000</td>\n",
       "      <td>-26.243000</td>\n",
       "      <td>-26.255000</td>\n",
       "      <td>-22.261000</td>\n",
       "      <td>24.432500</td>\n",
       "      <td>-26.184000</td>\n",
       "      <td>-26.182000</td>\n",
       "      <td>-26.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.550000</td>\n",
       "      <td>1.268000</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>15.638000</td>\n",
       "      <td>33.144000</td>\n",
       "      <td>17.153000</td>\n",
       "      <td>3.411000</td>\n",
       "      <td>-25.853000</td>\n",
       "      <td>-25.869000</td>\n",
       "      <td>-21.795000</td>\n",
       "      <td>24.902000</td>\n",
       "      <td>-25.798000</td>\n",
       "      <td>-25.792000</td>\n",
       "      <td>-25.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.098000</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>18.997000</td>\n",
       "      <td>36.491000</td>\n",
       "      <td>18.857000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-24.117000</td>\n",
       "      <td>-24.104000</td>\n",
       "      <td>-20.093000</td>\n",
       "      <td>26.579000</td>\n",
       "      <td>-24.151000</td>\n",
       "      <td>-24.117000</td>\n",
       "      <td>-24.137000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Y_01          Y_02          Y_03          Y_04          Y_05  \\\n",
       "count  35690.000000  35690.000000  35690.000000  35690.000000  35690.000000   \n",
       "mean       1.333359      1.034397      0.988477     13.722972     31.342709   \n",
       "std        0.305719      0.344410      0.315419      2.496548      2.409967   \n",
       "min        0.202000      0.201000      0.200000      7.002000     22.033000   \n",
       "25%        1.131000      0.790000      0.766000     11.986250     29.880250   \n",
       "50%        1.340000      1.030000      0.983000     13.909000     31.718000   \n",
       "75%        1.550000      1.268000      1.210000     15.638000     33.144000   \n",
       "max        2.000000      2.098000      2.080000     18.997000     36.491000   \n",
       "\n",
       "               Y_06          Y_07          Y_08          Y_09          Y_10  \\\n",
       "count  35690.000000  35690.000000  35690.000000  35690.000000  35690.000000   \n",
       "mean      16.643526      3.148128    -26.269909    -26.283590    -22.352684   \n",
       "std        0.723800      0.361532      0.623804      0.617551      0.788934   \n",
       "min       12.530000      2.400000    -29.129000    -29.010000    -26.475000   \n",
       "25%       16.199000      2.872000    -26.659000    -26.670750    -22.823000   \n",
       "50%       16.706000      3.121000    -26.243000    -26.255000    -22.261000   \n",
       "75%       17.153000      3.411000    -25.853000    -25.869000    -21.795000   \n",
       "max       18.857000      4.000000    -24.117000    -24.104000    -20.093000   \n",
       "\n",
       "               Y_11          Y_12          Y_13          Y_14  \n",
       "count  35690.000000  35690.000000  35690.000000  35690.000000  \n",
       "mean      24.357592    -26.212676    -26.208827    -26.220969  \n",
       "std        0.764020      0.619848      0.618885      0.619733  \n",
       "min       20.175000    -29.082000    -29.102000    -29.050000  \n",
       "25%       23.883000    -26.601000    -26.593000    -26.608000  \n",
       "50%       24.432500    -26.184000    -26.182000    -26.193000  \n",
       "75%       24.902000    -25.798000    -25.792000    -25.807000  \n",
       "max       26.579000    -24.151000    -24.117000    -24.137000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('normal_train.csv')\n",
    "df.filter(regex='Y').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bc01ed-abee-422e-9c30-023b2d0a44bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "      <td>3917.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.540192</td>\n",
       "      <td>1.265652</td>\n",
       "      <td>1.246568</td>\n",
       "      <td>12.693806</td>\n",
       "      <td>30.814463</td>\n",
       "      <td>15.489352</td>\n",
       "      <td>3.218161</td>\n",
       "      <td>-26.521984</td>\n",
       "      <td>-26.536706</td>\n",
       "      <td>-22.831751</td>\n",
       "      <td>24.028655</td>\n",
       "      <td>-26.466331</td>\n",
       "      <td>-26.462041</td>\n",
       "      <td>-26.472738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.626933</td>\n",
       "      <td>0.616249</td>\n",
       "      <td>0.595734</td>\n",
       "      <td>3.904389</td>\n",
       "      <td>3.497636</td>\n",
       "      <td>5.501632</td>\n",
       "      <td>0.761232</td>\n",
       "      <td>0.899505</td>\n",
       "      <td>0.887077</td>\n",
       "      <td>1.642800</td>\n",
       "      <td>1.246344</td>\n",
       "      <td>0.892844</td>\n",
       "      <td>0.889845</td>\n",
       "      <td>0.891526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>-0.331000</td>\n",
       "      <td>18.589000</td>\n",
       "      <td>-19.963000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>-29.652000</td>\n",
       "      <td>-29.523000</td>\n",
       "      <td>-31.119000</td>\n",
       "      <td>19.844000</td>\n",
       "      <td>-29.544000</td>\n",
       "      <td>-29.448000</td>\n",
       "      <td>-29.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.056000</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>9.983000</td>\n",
       "      <td>28.280000</td>\n",
       "      <td>15.409000</td>\n",
       "      <td>2.601000</td>\n",
       "      <td>-27.089000</td>\n",
       "      <td>-27.090000</td>\n",
       "      <td>-23.560000</td>\n",
       "      <td>23.138000</td>\n",
       "      <td>-27.026000</td>\n",
       "      <td>-27.027000</td>\n",
       "      <td>-27.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.543000</td>\n",
       "      <td>1.291000</td>\n",
       "      <td>1.259000</td>\n",
       "      <td>12.775000</td>\n",
       "      <td>31.540000</td>\n",
       "      <td>16.465000</td>\n",
       "      <td>3.232000</td>\n",
       "      <td>-26.395000</td>\n",
       "      <td>-26.429000</td>\n",
       "      <td>-22.498000</td>\n",
       "      <td>24.210000</td>\n",
       "      <td>-26.352000</td>\n",
       "      <td>-26.335000</td>\n",
       "      <td>-26.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.070000</td>\n",
       "      <td>1.716000</td>\n",
       "      <td>1.724000</td>\n",
       "      <td>15.470000</td>\n",
       "      <td>33.653000</td>\n",
       "      <td>17.313000</td>\n",
       "      <td>3.988000</td>\n",
       "      <td>-25.881000</td>\n",
       "      <td>-25.893000</td>\n",
       "      <td>-21.735000</td>\n",
       "      <td>25.048000</td>\n",
       "      <td>-25.818000</td>\n",
       "      <td>-25.819000</td>\n",
       "      <td>-25.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.409000</td>\n",
       "      <td>3.998000</td>\n",
       "      <td>3.756000</td>\n",
       "      <td>98.794000</td>\n",
       "      <td>37.250000</td>\n",
       "      <td>18.998000</td>\n",
       "      <td>5.299000</td>\n",
       "      <td>-23.785000</td>\n",
       "      <td>-23.960000</td>\n",
       "      <td>-20.052000</td>\n",
       "      <td>26.703000</td>\n",
       "      <td>-23.722000</td>\n",
       "      <td>-23.899000</td>\n",
       "      <td>-23.856000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Y_01         Y_02         Y_03         Y_04         Y_05  \\\n",
       "count  3917.000000  3917.000000  3917.000000  3917.000000  3917.000000   \n",
       "mean      1.540192     1.265652     1.246568    12.693806    30.814463   \n",
       "std       0.626933     0.616249     0.595734     3.904389     3.497636   \n",
       "min       0.017000     0.007000     0.017000    -0.331000    18.589000   \n",
       "25%       1.056000     0.834000     0.823000     9.983000    28.280000   \n",
       "50%       1.543000     1.291000     1.259000    12.775000    31.540000   \n",
       "75%       2.070000     1.716000     1.724000    15.470000    33.653000   \n",
       "max       4.409000     3.998000     3.756000    98.794000    37.250000   \n",
       "\n",
       "              Y_06         Y_07         Y_08         Y_09         Y_10  \\\n",
       "count  3917.000000  3917.000000  3917.000000  3917.000000  3917.000000   \n",
       "mean     15.489352     3.218161   -26.521984   -26.536706   -22.831751   \n",
       "std       5.501632     0.761232     0.899505     0.887077     1.642800   \n",
       "min     -19.963000     0.502000   -29.652000   -29.523000   -31.119000   \n",
       "25%      15.409000     2.601000   -27.089000   -27.090000   -23.560000   \n",
       "50%      16.465000     3.232000   -26.395000   -26.429000   -22.498000   \n",
       "75%      17.313000     3.988000   -25.881000   -25.893000   -21.735000   \n",
       "max      18.998000     5.299000   -23.785000   -23.960000   -20.052000   \n",
       "\n",
       "              Y_11         Y_12         Y_13         Y_14  \n",
       "count  3917.000000  3917.000000  3917.000000  3917.000000  \n",
       "mean     24.028655   -26.466331   -26.462041   -26.472738  \n",
       "std       1.246344     0.892844     0.889845     0.891526  \n",
       "min      19.844000   -29.544000   -29.448000   -29.620000  \n",
       "25%      23.138000   -27.026000   -27.027000   -27.034000  \n",
       "50%      24.210000   -26.352000   -26.335000   -26.361000  \n",
       "75%      25.048000   -25.818000   -25.819000   -25.827000  \n",
       "max      26.703000   -23.722000   -23.899000   -23.856000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('abnormal_train.csv')\n",
    "df.filter(regex='Y').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0cd5ca6-3679-4c28-9b9e-ea8df11392ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1297/4131828009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ex' is not defined"
     ]
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8333093-5002-4171-8c0e-99dbb8b567c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e8195-e563-4ca8-bdd0-5298a64b3c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89491f-ed3a-4612-b7ad-7205546bfa94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1550e91-b7ea-46b9-b174-96582267c59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b24da2-944b-4dba-8b1e-193b037dc984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382b3f44-5b9b-4c0f-96bb-3bfd05b59a58",
   "metadata": {},
   "source": [
    "### Abnormal과 Normal 데이터를 따로 학습 후 성능 비교\n",
    "= 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff598ee-1308-4e74-bc79-bf2cc2ab4721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_01 1193\n",
      "Y_02 450\n",
      "Y_03 382\n",
      "Y_04 400\n",
      "Y_05 73\n",
      "Y_06 8\n",
      "Y_07 1430\n",
      "Y_08 14\n",
      "Y_09 17\n",
      "Y_10 2\n",
      "Y_11 2\n",
      "Y_12 13\n",
      "Y_13 14\n",
      "Y_14 11\n"
     ]
    }
   ],
   "source": [
    "y_spec = pd.read_csv('./meta/y_feature_spec_info.csv')\n",
    "\n",
    "spec = {}\n",
    "for _, row in y_spec.iterrows():\n",
    "    spec[row.Feature] = (row.최소, row.최대)\n",
    "\n",
    "abnormal_data = defaultdict(list) # col : [abnormal]\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    for col, val in spec.items():\n",
    "        minval, maxval = val\n",
    "        if minval <= row[col] <= maxval:\n",
    "            continue\n",
    "        abnormal_data[col].append(idx)\n",
    "\n",
    "for col in sorted(abnormal_data):\n",
    "    print(col, len(abnormal_data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c3bd035-195e-4f64-8d83-cbd776e0097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38414\n",
      "Y_01 0.256701815599726 0.26494716738205604\n",
      "39157\n",
      "Y_02 0.3598565996859803 0.41324066972264484\n",
      "39225\n",
      "Y_03 0.35752967300800553 0.37155991606047195\n",
      "39207\n",
      "Y_04 0.19533562946901428 0.19390639089772216\n",
      "Y_05 0.08081274737080443\n",
      "Y_06 0.10260606249917295\n",
      "38177\n",
      "Y_07 0.12955151601378698 0.14734218646680006\n",
      "Y_08 0.02430583156247794\n",
      "Y_09 0.024633751790840014\n",
      "Y_10 0.03975551454084575\n",
      "Y_11 0.03436849990698669\n",
      "Y_12 0.024310799336858764\n",
      "Y_13 0.024790556483332643\n",
      "Y_14 0.02457961296515585\n",
      "\n",
      " ********** \n",
      "\n",
      "total | baseline : 2.0057658659056847 | nor : 1.980478585274782 | abr : 2.0909039013786\n",
      "Y_01 | baseline : 0.263815936718668 | nor : 0.256701815599726 | abr : 0.26494716738205604\n",
      "Y_02 | baseline : 0.3651145844355974 | nor : 0.3598565996859803 | abr : 0.41324066972264484\n",
      "Y_03 | baseline : 0.3597224977179273 | nor : 0.35752967300800553 | abr : 0.37155991606047195\n",
      "Y_04 | baseline : 0.19118359676732682 | nor : 0.19533562946901428 | abr : 0.19390639089772216\n",
      "Y_05 | baseline : 0.07961938614664572 | nor : 0.08081274737080443 | abr : 0.08081274737080443\n",
      "Y_06 | baseline : 0.11404120552786717 | nor : 0.10260606249917295 | abr : 0.10260606249917295\n",
      "Y_07 | baseline : 0.1298247508646181 | nor : 0.12955151601378698 | abr : 0.14734218646680006\n",
      "Y_08 | baseline : 0.024620529495198832 | nor : 0.02430583156247794 | abr : 0.02430583156247794\n",
      "Y_09 | baseline : 0.024431407359814288 | nor : 0.024633751790840014 | abr : 0.024633751790840014\n",
      "Y_10 | baseline : 0.04008515923409129 | nor : 0.03975551454084575 | abr : 0.03975551454084575\n",
      "Y_11 | baseline : 0.03399353979572566 | nor : 0.03436849990698669 | abr : 0.03436849990698669\n",
      "Y_12 | baseline : 0.024625581962635024 | nor : 0.024310799336858764 | abr : 0.024310799336858764\n",
      "Y_13 | baseline : 0.02449237270957379 | nor : 0.024790556483332643 | abr : 0.024790556483332643\n",
      "Y_14 | baseline : 0.02460681963522579 | nor : 0.02457961296515585 | abr : 0.02457961296515585\n",
      "\n",
      " ********** \n",
      "\n",
      "total | baseline : 2.0057658659056847 | exp : 2.035691243326691\n",
      "Y_01 | baseline : 0.263815936718668 | exp : 0.26082449149089104\n",
      "Y_02 | baseline : 0.3651145844355974 | exp : 0.38654863470431255\n",
      "Y_03 | baseline : 0.3597224977179273 | exp : 0.3645447945342387\n",
      "Y_04 | baseline : 0.19118359676732682 | exp : 0.19462101018336822\n",
      "Y_05 | baseline : 0.07961938614664572 | exp : 0.08081274737080443\n",
      "Y_06 | baseline : 0.11404120552786717 | exp : 0.10260606249917295\n",
      "Y_07 | baseline : 0.1298247508646181 | exp : 0.13844685124029352\n",
      "Y_08 | baseline : 0.024620529495198832 | exp : 0.02430583156247794\n",
      "Y_09 | baseline : 0.024431407359814288 | exp : 0.024633751790840014\n",
      "Y_10 | baseline : 0.04008515923409129 | exp : 0.03975551454084575\n",
      "Y_11 | baseline : 0.03399353979572566 | exp : 0.03436849990698669\n",
      "Y_12 | baseline : 0.024625581962635024 | exp : 0.024310799336858764\n",
      "Y_13 | baseline : 0.02449237270957379 | exp : 0.024790556483332643\n",
      "Y_14 | baseline : 0.02460681963522579 | exp : 0.02457961296515585\n"
     ]
    }
   ],
   "source": [
    "def total_score(score):\n",
    "    return sum(score[:8]) * 1.2 + sum(score[8:])\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "exp_score = {}\n",
    "nor_score = {}\n",
    "abr_score = {}\n",
    "\n",
    "for col in train_y.columns:\n",
    "    if col in ['Y_01', 'Y_02', 'Y_03', 'Y_04', 'Y_07']:\n",
    "        remove_idx = abnormal_data[col]\n",
    "        normal_df = df.loc[~df.index.isin(remove_idx)] #df.drop(df.index[remove_idx])\n",
    "        print(len(normal_df))\n",
    "        abnormal_df = df.loc[remove_idx]\n",
    "        \n",
    "        nrmse_normal = train_and_predict_single(normal_df, col)\n",
    "        nrmse_abnormal = train_and_predict_single(abnormal_df, col)\n",
    "        exp_score[col] = (nrmse_normal + nrmse_abnormal) / 2\n",
    "        nor_score[col] = nrmse_normal\n",
    "        abr_score[col] = nrmse_abnormal\n",
    "        print(col, nrmse_normal, nrmse_abnormal)\n",
    "    else:\n",
    "        nrmse = train_and_predict_single(df, col)\n",
    "        exp_score[col] = nor_score[col] = abr_score[col] = nrmse\n",
    "        print(col, nrmse)\n",
    "\n",
    "print('\\n', \"*\"*10, '\\n')\n",
    "\n",
    "print(f\"total | baseline : {total_score(list(baseline_score.values()))} | nor : {total_score(list(nor_score.values()))} | abr : {total_score(list(abr_score.values()))}\")\n",
    "for col in train_y.columns:\n",
    "    print(f\"{col} | baseline : {baseline_score[col]} | nor : {nor_score[col]} | abr : {abr_score[col]}\")\n",
    "\n",
    "print('\\n', \"*\"*10, '\\n')\n",
    "\n",
    "print(f\"total | baseline : {total_score(list(baseline_score.values()))} | exp : {total_score(list(exp_score.values()))}\")\n",
    "for col in train_y.columns:\n",
    "    print(f\"{col} | baseline : {baseline_score[col]} | exp : {exp_score[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b1e81-ee34-44c7-94d1-8873f38c09fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.492839</td>\n",
       "      <td>1.208765</td>\n",
       "      <td>1.188903</td>\n",
       "      <td>12.751931</td>\n",
       "      <td>30.747018</td>\n",
       "      <td>15.617110</td>\n",
       "      <td>3.173987</td>\n",
       "      <td>-26.562992</td>\n",
       "      <td>-26.586592</td>\n",
       "      <td>-22.831125</td>\n",
       "      <td>23.992714</td>\n",
       "      <td>-26.499217</td>\n",
       "      <td>-26.503821</td>\n",
       "      <td>-26.496311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.598263</td>\n",
       "      <td>0.583126</td>\n",
       "      <td>0.573141</td>\n",
       "      <td>3.616121</td>\n",
       "      <td>3.532006</td>\n",
       "      <td>5.029925</td>\n",
       "      <td>0.753999</td>\n",
       "      <td>0.887745</td>\n",
       "      <td>0.894197</td>\n",
       "      <td>1.564185</td>\n",
       "      <td>1.263120</td>\n",
       "      <td>0.889725</td>\n",
       "      <td>0.890145</td>\n",
       "      <td>0.887844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>3.312000</td>\n",
       "      <td>20.825000</td>\n",
       "      <td>-18.926000</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>-29.330000</td>\n",
       "      <td>-29.331000</td>\n",
       "      <td>-30.548000</td>\n",
       "      <td>20.334000</td>\n",
       "      <td>-29.131000</td>\n",
       "      <td>-29.443000</td>\n",
       "      <td>-29.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.022500</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.811750</td>\n",
       "      <td>10.140750</td>\n",
       "      <td>28.162750</td>\n",
       "      <td>15.448250</td>\n",
       "      <td>2.469250</td>\n",
       "      <td>-27.115000</td>\n",
       "      <td>-27.158250</td>\n",
       "      <td>-23.521250</td>\n",
       "      <td>23.168750</td>\n",
       "      <td>-27.050000</td>\n",
       "      <td>-27.061250</td>\n",
       "      <td>-27.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.491000</td>\n",
       "      <td>1.211000</td>\n",
       "      <td>1.197500</td>\n",
       "      <td>13.212500</td>\n",
       "      <td>31.533500</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>3.165500</td>\n",
       "      <td>-26.461000</td>\n",
       "      <td>-26.489000</td>\n",
       "      <td>-22.517500</td>\n",
       "      <td>24.139000</td>\n",
       "      <td>-26.402000</td>\n",
       "      <td>-26.420000</td>\n",
       "      <td>-26.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.043000</td>\n",
       "      <td>1.623000</td>\n",
       "      <td>1.671250</td>\n",
       "      <td>15.388000</td>\n",
       "      <td>33.574750</td>\n",
       "      <td>17.281750</td>\n",
       "      <td>3.886500</td>\n",
       "      <td>-25.891000</td>\n",
       "      <td>-25.907500</td>\n",
       "      <td>-21.772750</td>\n",
       "      <td>25.048250</td>\n",
       "      <td>-25.846500</td>\n",
       "      <td>-25.832000</td>\n",
       "      <td>-25.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.866000</td>\n",
       "      <td>2.627000</td>\n",
       "      <td>2.753000</td>\n",
       "      <td>19.968000</td>\n",
       "      <td>37.101000</td>\n",
       "      <td>18.679000</td>\n",
       "      <td>4.868000</td>\n",
       "      <td>-24.618000</td>\n",
       "      <td>-24.770000</td>\n",
       "      <td>-20.235000</td>\n",
       "      <td>26.314000</td>\n",
       "      <td>-24.725000</td>\n",
       "      <td>-24.543000</td>\n",
       "      <td>-24.753000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y_01        Y_02        Y_03        Y_04        Y_05        Y_06  \\\n",
       "count  392.000000  392.000000  392.000000  392.000000  392.000000  392.000000   \n",
       "mean     1.492839    1.208765    1.188903   12.751931   30.747018   15.617110   \n",
       "std      0.598263    0.583126    0.573141    3.616121    3.532006    5.029925   \n",
       "min      0.028000    0.012000    0.029000    3.312000   20.825000  -18.926000   \n",
       "25%      1.022500    0.778000    0.811750   10.140750   28.162750   15.448250   \n",
       "50%      1.491000    1.211000    1.197500   13.212500   31.533500   16.400000   \n",
       "75%      2.043000    1.623000    1.671250   15.388000   33.574750   17.281750   \n",
       "max      2.866000    2.627000    2.753000   19.968000   37.101000   18.679000   \n",
       "\n",
       "             Y_07        Y_08        Y_09        Y_10        Y_11        Y_12  \\\n",
       "count  392.000000  392.000000  392.000000  392.000000  392.000000  392.000000   \n",
       "mean     3.173987  -26.562992  -26.586592  -22.831125   23.992714  -26.499217   \n",
       "std      0.753999    0.887745    0.894197    1.564185    1.263120    0.889725   \n",
       "min      0.921000  -29.330000  -29.331000  -30.548000   20.334000  -29.131000   \n",
       "25%      2.469250  -27.115000  -27.158250  -23.521250   23.168750  -27.050000   \n",
       "50%      3.165500  -26.461000  -26.489000  -22.517500   24.139000  -26.402000   \n",
       "75%      3.886500  -25.891000  -25.907500  -21.772750   25.048250  -25.846500   \n",
       "max      4.868000  -24.618000  -24.770000  -20.235000   26.314000  -24.725000   \n",
       "\n",
       "             Y_13        Y_14  \n",
       "count  392.000000  392.000000  \n",
       "mean   -26.503821  -26.496311  \n",
       "std      0.890145    0.887844  \n",
       "min    -29.443000  -29.340000  \n",
       "25%    -27.061250  -27.002000  \n",
       "50%    -26.420000  -26.370500  \n",
       "75%    -25.832000  -25.856000  \n",
       "max    -24.543000  -24.753000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.filter(regex='Y').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60b201a8-ebbe-4ec5-bd22-7a7762981479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.544879</td>\n",
       "      <td>1.270445</td>\n",
       "      <td>1.251040</td>\n",
       "      <td>12.786463</td>\n",
       "      <td>30.862474</td>\n",
       "      <td>15.449166</td>\n",
       "      <td>3.210839</td>\n",
       "      <td>-26.507107</td>\n",
       "      <td>-26.519935</td>\n",
       "      <td>-22.826547</td>\n",
       "      <td>24.038244</td>\n",
       "      <td>-26.452408</td>\n",
       "      <td>-26.447485</td>\n",
       "      <td>-26.461649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161904</td>\n",
       "      <td>0.158420</td>\n",
       "      <td>0.140753</td>\n",
       "      <td>1.114050</td>\n",
       "      <td>0.991388</td>\n",
       "      <td>1.760799</td>\n",
       "      <td>0.258701</td>\n",
       "      <td>0.244287</td>\n",
       "      <td>0.227791</td>\n",
       "      <td>0.475788</td>\n",
       "      <td>0.299157</td>\n",
       "      <td>0.236312</td>\n",
       "      <td>0.235655</td>\n",
       "      <td>0.232709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.711457</td>\n",
       "      <td>0.105998</td>\n",
       "      <td>0.218271</td>\n",
       "      <td>10.061627</td>\n",
       "      <td>28.326014</td>\n",
       "      <td>10.339852</td>\n",
       "      <td>2.455178</td>\n",
       "      <td>-27.338167</td>\n",
       "      <td>-27.106776</td>\n",
       "      <td>-24.759388</td>\n",
       "      <td>23.084367</td>\n",
       "      <td>-27.061971</td>\n",
       "      <td>-27.068175</td>\n",
       "      <td>-27.182945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.429212</td>\n",
       "      <td>1.178219</td>\n",
       "      <td>1.166458</td>\n",
       "      <td>11.970589</td>\n",
       "      <td>30.185354</td>\n",
       "      <td>14.307900</td>\n",
       "      <td>3.042128</td>\n",
       "      <td>-26.673171</td>\n",
       "      <td>-26.674559</td>\n",
       "      <td>-23.162649</td>\n",
       "      <td>23.839271</td>\n",
       "      <td>-26.621875</td>\n",
       "      <td>-26.615158</td>\n",
       "      <td>-26.626221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.542291</td>\n",
       "      <td>1.267446</td>\n",
       "      <td>1.246321</td>\n",
       "      <td>12.748371</td>\n",
       "      <td>31.001131</td>\n",
       "      <td>15.646355</td>\n",
       "      <td>3.223269</td>\n",
       "      <td>-26.497517</td>\n",
       "      <td>-26.511696</td>\n",
       "      <td>-22.844119</td>\n",
       "      <td>24.059362</td>\n",
       "      <td>-26.434684</td>\n",
       "      <td>-26.430501</td>\n",
       "      <td>-26.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.658613</td>\n",
       "      <td>1.379813</td>\n",
       "      <td>1.344828</td>\n",
       "      <td>13.599288</td>\n",
       "      <td>31.520215</td>\n",
       "      <td>16.817961</td>\n",
       "      <td>3.383375</td>\n",
       "      <td>-26.333068</td>\n",
       "      <td>-26.370022</td>\n",
       "      <td>-22.513304</td>\n",
       "      <td>24.242375</td>\n",
       "      <td>-26.292286</td>\n",
       "      <td>-26.286134</td>\n",
       "      <td>-26.298227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.937013</td>\n",
       "      <td>1.661163</td>\n",
       "      <td>1.597667</td>\n",
       "      <td>16.802252</td>\n",
       "      <td>33.986665</td>\n",
       "      <td>19.402999</td>\n",
       "      <td>3.879995</td>\n",
       "      <td>-25.849695</td>\n",
       "      <td>-25.881558</td>\n",
       "      <td>-21.329897</td>\n",
       "      <td>24.862362</td>\n",
       "      <td>-25.777394</td>\n",
       "      <td>-25.793268</td>\n",
       "      <td>-25.826442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y_01        Y_02        Y_03        Y_04        Y_05        Y_06  \\\n",
       "count  392.000000  392.000000  392.000000  392.000000  392.000000  392.000000   \n",
       "mean     1.544879    1.270445    1.251040   12.786463   30.862474   15.449166   \n",
       "std      0.161904    0.158420    0.140753    1.114050    0.991388    1.760799   \n",
       "min      0.711457    0.105998    0.218271   10.061627   28.326014   10.339852   \n",
       "25%      1.429212    1.178219    1.166458   11.970589   30.185354   14.307900   \n",
       "50%      1.542291    1.267446    1.246321   12.748371   31.001131   15.646355   \n",
       "75%      1.658613    1.379813    1.344828   13.599288   31.520215   16.817961   \n",
       "max      1.937013    1.661163    1.597667   16.802252   33.986665   19.402999   \n",
       "\n",
       "             Y_07        Y_08        Y_09        Y_10        Y_11        Y_12  \\\n",
       "count  392.000000  392.000000  392.000000  392.000000  392.000000  392.000000   \n",
       "mean     3.210839  -26.507107  -26.519935  -22.826547   24.038244  -26.452408   \n",
       "std      0.258701    0.244287    0.227791    0.475788    0.299157    0.236312   \n",
       "min      2.455178  -27.338167  -27.106776  -24.759388   23.084367  -27.061971   \n",
       "25%      3.042128  -26.673171  -26.674559  -23.162649   23.839271  -26.621875   \n",
       "50%      3.223269  -26.497517  -26.511696  -22.844119   24.059362  -26.434684   \n",
       "75%      3.383375  -26.333068  -26.370022  -22.513304   24.242375  -26.292286   \n",
       "max      3.879995  -25.849695  -25.881558  -21.329897   24.862362  -25.777394   \n",
       "\n",
       "             Y_13        Y_14  \n",
       "count  392.000000  392.000000  \n",
       "mean   -26.447485  -26.461649  \n",
       "std      0.235655    0.232709  \n",
       "min    -27.068175  -27.182945  \n",
       "25%    -26.615158  -26.626221  \n",
       "50%    -26.430501  -26.436900  \n",
       "75%    -26.286134  -26.298227  \n",
       "max    -25.793268  -25.826442  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(valid_preds, columns=valid_df.filter(regex='Y').columns).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac4d88-ce66-410c-bd60-23cd17a2dc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614dbdcb-cd03-4191-8c2d-f7f10e73a7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9589ac-d25f-41ef-ac89-62c085827585",
   "metadata": {},
   "source": [
    "### Abnormal 데이터를 최소 abnormal과 최대 abnormal로 나누어 따로 학습\n",
    "- 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39654ee8-3ba5-49e1-a1cd-8ae93f95657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_01 78 1108\n",
      "Y_02 195 247\n",
      "Y_03 200 170\n",
      "Y_04 281 116\n",
      "Y_05 55 23\n",
      "Y_06 9 0\n",
      "Y_07 704 775\n",
      "Y_08 17 0\n",
      "Y_09 16 0\n",
      "Y_10 4 0\n",
      "Y_11 0 2\n",
      "Y_12 12 1\n",
      "Y_13 12 1\n",
      "Y_14 11 0\n"
     ]
    }
   ],
   "source": [
    "y_spec = pd.read_csv('./meta/y_feature_spec_info.csv')\n",
    "\n",
    "spec = {}\n",
    "for _, row in y_spec.iterrows():\n",
    "    spec[row.Feature] = (row.최소, row.최대)\n",
    "\n",
    "min_abnormal_data = defaultdict(list) # col : [abnormal]\n",
    "max_abnormal_data = defaultdict(list) # col : [abnormal]\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    for col, val in spec.items():\n",
    "        minval, maxval = val\n",
    "        if minval <= row[col] <= maxval:\n",
    "            continue\n",
    "        if minval > row[col]:\n",
    "            min_abnormal_data[col].append(idx)\n",
    "        if maxval < row[col]:\n",
    "            max_abnormal_data[col].append(idx)\n",
    "\n",
    "for col in sorted(abnormal_data):\n",
    "    print(col, len(min_abnormal_data[col]), len(max_abnormal_data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215ac934-4884-4c88-8798-6dff32c27fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38421\n",
      "Y_01 0.26722455780568455 0.5383015199232745 0.2773538391946826\n",
      "39165\n",
      "Y_02 0.35594144686482265 0.38819561916507095 0.4112087977167971\n",
      "39237\n",
      "Y_03 0.3474596637474507 0.39715078450887004 0.3255134742432353\n",
      "39210\n",
      "Y_04 0.19386212899568048 0.21325501619316511 0.2621400358084145\n",
      "Y_05 0.07961860554602941\n",
      "Y_06 0.09991694488062193\n",
      "38128\n",
      "Y_07 0.1288936026208873 0.16551228117851438 0.1325954746713617\n",
      "Y_08 0.025150936860349783\n",
      "Y_09 0.024918198906955926\n",
      "Y_10 0.03997909949865625\n",
      "Y_11 0.033655480215251436\n",
      "Y_12 0.02404691290749001\n",
      "Y_13 0.024556854022682102\n",
      "Y_14 0.024124903134946037\n",
      "\n",
      " ********** \n",
      "\n",
      "total | baseline : 1.9460990550539288 | nor : 1.968962913471814 | min_abr : 2.459803498593057 | max_abr : 2.1074791793917727\n",
      "Y_01 | baseline : 0.25607406817106754 | nor : 0.26722455780568455 | min_abr : 0.5383015199232745 | max_abr : 0.2773538391946826\n",
      "Y_02 | baseline : 0.36027038009063533 | nor : 0.35594144686482265 | min_abr : 0.38819561916507095 | max_abr : 0.4112087977167971\n",
      "Y_03 | baseline : 0.3499151122219958 | nor : 0.3474596637474507 | min_abr : 0.39715078450887004 | max_abr : 0.3255134742432353\n",
      "Y_04 | baseline : 0.19051266247721013 | nor : 0.19386212899568048 | min_abr : 0.21325501619316511 | max_abr : 0.2621400358084145\n",
      "Y_05 | baseline : 0.0797962126817236 | nor : 0.07961860554602941 | min_abr : 0.07961860554602941 | max_abr : 0.07961860554602941\n",
      "Y_06 | baseline : 0.09027330615523828 | nor : 0.09991694488062193 | min_abr : 0.09991694488062193 | max_abr : 0.09991694488062193\n",
      "Y_07 | baseline : 0.13002004609441625 | nor : 0.1288936026208873 | min_abr : 0.16551228117851438 | max_abr : 0.1325954746713617\n",
      "Y_08 | baseline : 0.024401919157624935 | nor : 0.025150936860349783 | min_abr : 0.025150936860349783 | max_abr : 0.025150936860349783\n",
      "Y_09 | baseline : 0.024092726801144635 | nor : 0.024918198906955926 | min_abr : 0.024918198906955926 | max_abr : 0.024918198906955926\n",
      "Y_10 | baseline : 0.038120207428351816 | nor : 0.03997909949865625 | min_abr : 0.03997909949865625 | max_abr : 0.03997909949865625\n",
      "Y_11 | baseline : 0.033639334044133315 | nor : 0.033655480215251436 | min_abr : 0.033655480215251436 | max_abr : 0.033655480215251436\n",
      "Y_12 | baseline : 0.024227637040025403 | nor : 0.02404691290749001 | min_abr : 0.02404691290749001 | max_abr : 0.02404691290749001\n",
      "Y_13 | baseline : 0.024184798538468125 | nor : 0.024556854022682102 | min_abr : 0.024556854022682102 | max_abr : 0.024556854022682102\n",
      "Y_14 | baseline : 0.024317902741911236 | nor : 0.024124903134946037 | min_abr : 0.024124903134946037 | max_abr : 0.024124903134946037\n",
      "\n",
      " ********** \n",
      "\n",
      "total | baseline : 1.9460990550539288 | exp : 2.1787485304855476\n",
      "Y_01 | baseline : 0.25607406817106754 | exp : 0.3609599723078805\n",
      "Y_02 | baseline : 0.36027038009063533 | exp : 0.38511528791556354\n",
      "Y_03 | baseline : 0.3499151122219958 | exp : 0.35670797416651867\n",
      "Y_04 | baseline : 0.19051266247721013 | exp : 0.2230857269990867\n",
      "Y_05 | baseline : 0.0797962126817236 | exp : 0.07961860554602941\n",
      "Y_06 | baseline : 0.09027330615523828 | exp : 0.09991694488062193\n",
      "Y_07 | baseline : 0.13002004609441625 | exp : 0.14233378615692113\n",
      "Y_08 | baseline : 0.024401919157624935 | exp : 0.025150936860349783\n",
      "Y_09 | baseline : 0.024092726801144635 | exp : 0.024918198906955926\n",
      "Y_10 | baseline : 0.038120207428351816 | exp : 0.03997909949865625\n",
      "Y_11 | baseline : 0.033639334044133315 | exp : 0.033655480215251436\n",
      "Y_12 | baseline : 0.024227637040025403 | exp : 0.02404691290749001\n",
      "Y_13 | baseline : 0.024184798538468125 | exp : 0.024556854022682102\n",
      "Y_14 | baseline : 0.024317902741911236 | exp : 0.024124903134946037\n"
     ]
    }
   ],
   "source": [
    "def total_score(score):\n",
    "    return sum(score[:8]) * 1.2 + sum(score[8:])\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "exp_score = {}\n",
    "nor_score = {}\n",
    "min_abr_score = {}\n",
    "max_abr_score = {}\n",
    "\n",
    "for col in train_y.columns:\n",
    "    if col in ['Y_01', 'Y_02', 'Y_03', 'Y_04', 'Y_07']:\n",
    "        min_remove_idx = min_abnormal_data[col]\n",
    "        max_remove_idx = max_abnormal_data[col]\n",
    "        remove_idx = min_remove_idx + max_remove_idx\n",
    "        \n",
    "        normal_df = df.loc[~df.index.isin(remove_idx)] #df.drop(df.index[remove_idx])\n",
    "        print(len(normal_df))\n",
    "        min_abnormal_df = df.loc[min_remove_idx]\n",
    "        max_abnormal_df = df.loc[max_remove_idx]\n",
    "        \n",
    "        nrmse_normal = train_and_predict_single(normal_df, col)\n",
    "        nrmse_min_abnormal = train_and_predict_single(min_abnormal_df, col)\n",
    "        nrmse_max_abnormal = train_and_predict_single(max_abnormal_df, col)\n",
    "        \n",
    "        exp_score[col] = (nrmse_normal + nrmse_min_abnormal + nrmse_max_abnormal) / 3\n",
    "        nor_score[col] = nrmse_normal\n",
    "        min_abr_score[col] = nrmse_min_abnormal\n",
    "        max_abr_score[col] = nrmse_max_abnormal\n",
    "        \n",
    "        print(col, nrmse_normal, nrmse_min_abnormal, nrmse_max_abnormal)\n",
    "    else:\n",
    "        nrmse = train_and_predict_single(df, col)\n",
    "        exp_score[col] = nor_score[col] = min_abr_score[col] = max_abr_score[col] = nrmse\n",
    "        print(col, nrmse)\n",
    "\n",
    "print('\\n', \"*\"*10, '\\n')\n",
    "\n",
    "print(f\"total | baseline : {total_score(list(baseline_score.values()))} | nor : {total_score(list(nor_score.values()))} | min_abr : {total_score(list(min_abr_score.values()))} | max_abr : {total_score(list(max_abr_score.values()))}\")\n",
    "for col in train_y.columns:\n",
    "    print(f\"{col} | baseline : {baseline_score[col]} | nor : {nor_score[col]} | min_abr : {min_abr_score[col]} | max_abr : {max_abr_score[col]}\")\n",
    "\n",
    "print('\\n', \"*\"*10, '\\n')\n",
    "\n",
    "print(f\"total | baseline : {total_score(list(baseline_score.values()))} | exp : {total_score(list(exp_score.values()))}\")\n",
    "for col in train_y.columns:\n",
    "    print(f\"{col} | baseline : {baseline_score[col]} | exp : {exp_score[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaba444-f60b-456a-a2f1-f257ebc5ab0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcdd60a-a5ee-47d3-9c01-e7972cc04e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca7f398-4e23-4a39-820e-7b49a38eabde",
   "metadata": {},
   "source": [
    "## AutoEncoder로 정상 비정상 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e6d834-0bfc-4e78-a63f-f7248417ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import collections\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score)\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "                            \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffdce6d0-d664-4f0a-a8f1-41848473c1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdd8b1d3-03d9-46dc-b295-e177bb33b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, length):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(length,20)\n",
    "        self.lin2 = nn.Linear(20,10)\n",
    "        self.lin7 = nn.Linear(10,20)\n",
    "        self.lin8 = nn.Linear(20,length)\n",
    "        \n",
    "        self.drop2 = nn.Dropout(0.05)\n",
    "        \n",
    "        self.lin1.weight.data.uniform_(-2,2)\n",
    "        self.lin2.weight.data.uniform_(-2,2)\n",
    "        self.lin7.weight.data.uniform_(-2,2)\n",
    "        self.lin8.weight.data.uniform_(-2,2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = F.tanh(self.lin1(data))\n",
    "        x = self.drop2(F.tanh(self.lin2(x)))\n",
    "        x = F.tanh(self.lin7(x))\n",
    "        x = self.lin8(x)\n",
    "        return x\n",
    "    \n",
    "def score(x):\n",
    "    y_pred = model(V(x))\n",
    "    x1 = V(x)\n",
    "    return loss(y_pred,x1).item()\n",
    "\n",
    "def get_pred(x):\n",
    "    y_pred = model(V(x))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5246adf-6fbb-4c31-bc6c-b2192c982c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anomaly_detection_train.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(df, train_size=0.8)\n",
    "\n",
    "train_x = train_df.filter(regex='X') # Input : X Featrue\n",
    "train_y = train_df.Abnormal # Output : Y Feature\n",
    "\n",
    "valid_x = valid_df.filter(regex='X') # Input : X Featrue\n",
    "valid_y = valid_df.Abnormal # Output : Y Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4f45ffd-f85c-4c03-88f1-0be1cf1957f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train_x = torch.FloatTensor(train_x.values)\n",
    "ts_valid_x = torch.FloatTensor(valid_x.values)\n",
    "\n",
    "xdl = DataLoader(ts_train_x, batch_size=1000)\n",
    "vdl = DataLoader(ts_valid_x, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd15a14c-f6f5-4399-88f2-38aebe823fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (lin1): Linear(in_features=56, out_features=20, bias=True)\n",
      "  (lin2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (lin7): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (lin8): Linear(in_features=20, out_features=56, bias=True)\n",
      "  (drop2): Dropout(p=0.05, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(len(train_x.columns))\n",
    "loss=nn.MSELoss()\n",
    "learning_rate = 3e-1\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b85b88b-4432-48b8-8827-73ce6fbb462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize a named tuple to keep track of scores at each epoch\n",
    "model_hist = collections.namedtuple('Model','epoch loss val_loss')\n",
    "model_loss = model_hist(epoch = [], loss = [], val_loss = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fd34dcd-fbf6-4519-b5cf-c53d327e3f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13635/1197001179.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(epochs),position=0, total = epochs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b29648ce47451c8a736e935486014d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Loss: 6636224.5000    Val_Loss: 5995836.8125\n",
      "Epoch: 1   Loss: 6508273.0000    Val_Loss: 5870667.5000\n",
      "Epoch: 2   Loss: 6384634.0000    Val_Loss: 5749767.5000\n",
      "Epoch: 3   Loss: 6264807.5000    Val_Loss: 5632647.8750\n",
      "Epoch: 4   Loss: 6148414.5000    Val_Loss: 5518934.8125\n",
      "Epoch: 5   Loss: 6035140.5000    Val_Loss: 5408313.3750\n",
      "Epoch: 6   Loss: 5924707.5000    Val_Loss: 5300510.0000\n",
      "Epoch: 7   Loss: 5816882.0000    Val_Loss: 5195290.1875\n",
      "Epoch: 8   Loss: 5711464.5000    Val_Loss: 5092458.1875\n",
      "Epoch: 9   Loss: 5608297.0000    Val_Loss: 4991855.0000\n",
      "Epoch: 10   Loss: 5507252.0000    Val_Loss: 4893355.8125\n",
      "Epoch: 11   Loss: 5408233.0000    Val_Loss: 4796864.3750\n",
      "Epoch: 12   Loss: 5311165.5000    Val_Loss: 4702306.7500\n",
      "Epoch: 13   Loss: 5215993.5000    Val_Loss: 4609627.4375\n",
      "Epoch: 14   Loss: 5122671.0000    Val_Loss: 4518781.9375\n",
      "Epoch: 15   Loss: 5031163.0000    Val_Loss: 4429733.4688\n",
      "Epoch: 16   Loss: 4941437.5000    Val_Loss: 4342452.4688\n",
      "Epoch: 17   Loss: 4853467.0000    Val_Loss: 4256909.8125\n",
      "Epoch: 18   Loss: 4767225.5000    Val_Loss: 4173080.5000\n",
      "Epoch: 19   Loss: 4682688.0000    Val_Loss: 4090940.1250\n",
      "Epoch: 20   Loss: 4599832.5000    Val_Loss: 4010464.3750\n",
      "Epoch: 21   Loss: 4518633.0000    Val_Loss: 3931630.5312\n",
      "Epoch: 22   Loss: 4439068.5000    Val_Loss: 3854414.9688\n",
      "Epoch: 23   Loss: 4361116.0000    Val_Loss: 3778796.2812\n",
      "Epoch: 24   Loss: 4284753.5000    Val_Loss: 3704752.0312\n",
      "Epoch: 25   Loss: 4209958.5000    Val_Loss: 3632260.1562\n",
      "Epoch: 26   Loss: 4136709.7500    Val_Loss: 3561298.2188\n",
      "Epoch: 27   Loss: 4064987.5000    Val_Loss: 3491846.5938\n",
      "Epoch: 28   Loss: 3994767.7500    Val_Loss: 3423883.0000\n",
      "Epoch: 29   Loss: 3926031.0000    Val_Loss: 3357387.2188\n",
      "Epoch: 30   Loss: 3858755.7500    Val_Loss: 3292337.9062\n",
      "Epoch: 31   Loss: 3792924.2500    Val_Loss: 3228715.4688\n",
      "Epoch: 32   Loss: 3728514.0000    Val_Loss: 3166499.4375\n",
      "Epoch: 33   Loss: 3665505.7500    Val_Loss: 3105669.5625\n",
      "Epoch: 34   Loss: 3603879.5000    Val_Loss: 3046206.9375\n",
      "Epoch: 35   Loss: 3543616.0000    Val_Loss: 2988091.2500\n",
      "Epoch: 36   Loss: 3484695.7500    Val_Loss: 2931303.1250\n",
      "Epoch: 37   Loss: 3427099.5000    Val_Loss: 2875824.2500\n",
      "Epoch: 38   Loss: 3370808.7500    Val_Loss: 2821634.6562\n",
      "Epoch: 39   Loss: 3315804.2500    Val_Loss: 2768716.2500\n",
      "Epoch: 40   Loss: 3262067.7500    Val_Loss: 2717050.2500\n",
      "Epoch: 41   Loss: 3209580.7500    Val_Loss: 2666618.4062\n",
      "Epoch: 42   Loss: 3158325.2500    Val_Loss: 2617402.0938\n",
      "Epoch: 43   Loss: 3108282.7500    Val_Loss: 2569383.5938\n",
      "Epoch: 44   Loss: 3059434.7500    Val_Loss: 2522544.0000\n",
      "Epoch: 45   Loss: 3011765.0000    Val_Loss: 2476866.6875\n",
      "Epoch: 46   Loss: 2965255.0000    Val_Loss: 2432333.3125\n",
      "Epoch: 47   Loss: 2919886.7500    Val_Loss: 2388927.0469\n",
      "Epoch: 48   Loss: 2875643.7500    Val_Loss: 2346629.4688\n",
      "Epoch: 49   Loss: 2832508.2500    Val_Loss: 2305423.6406\n",
      "Epoch: 50   Loss: 2790463.5000    Val_Loss: 2265292.9375\n",
      "Epoch: 51   Loss: 2749492.5000    Val_Loss: 2226220.2344\n",
      "Epoch: 52   Loss: 2709578.0000    Val_Loss: 2188188.2656\n",
      "Epoch: 53   Loss: 2670704.0000    Val_Loss: 2151180.7031\n",
      "Epoch: 54   Loss: 2632853.7500    Val_Loss: 2115180.0781\n",
      "Epoch: 55   Loss: 2596009.7500    Val_Loss: 2080170.7500\n",
      "Epoch: 56   Loss: 2560156.7500    Val_Loss: 2046136.0469\n",
      "Epoch: 57   Loss: 2525278.5000    Val_Loss: 2013059.3438\n",
      "Epoch: 58   Loss: 2491357.5000    Val_Loss: 1980924.9688\n",
      "Epoch: 59   Loss: 2458379.5000    Val_Loss: 1949715.9531\n",
      "Epoch: 60   Loss: 2426327.0000    Val_Loss: 1919416.5469\n",
      "Epoch: 61   Loss: 2395185.0000    Val_Loss: 1890011.6094\n",
      "Epoch: 62   Loss: 2364938.2500    Val_Loss: 1861484.4219\n",
      "Epoch: 63   Loss: 2335569.2500    Val_Loss: 1833818.8281\n",
      "Epoch: 64   Loss: 2307063.2500    Val_Loss: 1806999.9531\n",
      "Epoch: 65   Loss: 2279405.7500    Val_Loss: 1781012.0625\n",
      "Epoch: 66   Loss: 2252580.0000    Val_Loss: 1755840.0156\n",
      "Epoch: 67   Loss: 2226570.7500    Val_Loss: 1731467.1094\n",
      "Epoch: 68   Loss: 2201363.0000    Val_Loss: 1707878.7500\n",
      "Epoch: 69   Loss: 2176942.0000    Val_Loss: 1685060.0156\n",
      "Epoch: 70   Loss: 2153292.0000    Val_Loss: 1662995.2031\n",
      "Epoch: 71   Loss: 2130398.0000    Val_Loss: 1641669.3906\n",
      "Epoch: 72   Loss: 2108245.5000    Val_Loss: 1621067.6094\n",
      "Epoch: 73   Loss: 2086819.7500    Val_Loss: 1601174.6562\n",
      "Epoch: 74   Loss: 2066104.8750    Val_Loss: 1581975.8125\n",
      "Epoch: 75   Loss: 2046087.1250    Val_Loss: 1563455.9219\n",
      "Epoch: 76   Loss: 2026751.3750    Val_Loss: 1545600.8125\n",
      "Epoch: 77   Loss: 2008083.3750    Val_Loss: 1528395.4062\n",
      "Epoch: 78   Loss: 1990068.2500    Val_Loss: 1511824.8750\n",
      "Epoch: 79   Loss: 1972691.8750    Val_Loss: 1495875.4219\n",
      "Epoch: 80   Loss: 1955939.5000    Val_Loss: 1480531.6875\n",
      "Epoch: 81   Loss: 1939797.7500    Val_Loss: 1465780.1250\n",
      "Epoch: 82   Loss: 1924251.6250    Val_Loss: 1451606.4844\n",
      "Epoch: 83   Loss: 1909287.0000    Val_Loss: 1437996.0000\n",
      "Epoch: 84   Loss: 1894890.5000    Val_Loss: 1424934.8750\n",
      "Epoch: 85   Loss: 1881047.8750    Val_Loss: 1412408.9219\n",
      "Epoch: 86   Loss: 1867745.0000    Val_Loss: 1400404.7031\n",
      "Epoch: 87   Loss: 1854968.8750    Val_Loss: 1388907.9844\n",
      "Epoch: 88   Loss: 1842705.3750    Val_Loss: 1377905.2812\n",
      "Epoch: 89   Loss: 1830941.0000    Val_Loss: 1367382.8594\n",
      "Epoch: 90   Loss: 1819662.2500    Val_Loss: 1357327.2344\n",
      "Epoch: 91   Loss: 1808856.0000    Val_Loss: 1347724.8125\n",
      "Epoch: 92   Loss: 1798509.1250    Val_Loss: 1338562.8125\n",
      "Epoch: 93   Loss: 1788608.2500    Val_Loss: 1329827.3438\n",
      "Epoch: 94   Loss: 1779139.8750    Val_Loss: 1321506.0156\n",
      "Epoch: 95   Loss: 1770092.3750    Val_Loss: 1313585.4375\n",
      "Epoch: 96   Loss: 1761452.7500    Val_Loss: 1306053.3125\n",
      "Epoch: 97   Loss: 1753207.8750    Val_Loss: 1298896.7031\n",
      "Epoch: 98   Loss: 1745345.7500    Val_Loss: 1292103.2344\n",
      "Epoch: 99   Loss: 1737853.7500    Val_Loss: 1285660.2188\n",
      "Epoch: 100   Loss: 1730719.3750    Val_Loss: 1279555.5312\n",
      "Epoch: 101   Loss: 1723931.6250    Val_Loss: 1273777.3047\n",
      "Epoch: 102   Loss: 1717478.0000    Val_Loss: 1268314.0469\n",
      "Epoch: 103   Loss: 1711346.8750    Val_Loss: 1263153.4922\n",
      "Epoch: 104   Loss: 1705526.8750    Val_Loss: 1258284.2734\n",
      "Epoch: 105   Loss: 1700006.3750    Val_Loss: 1253694.9766\n",
      "Epoch: 106   Loss: 1694774.3750    Val_Loss: 1249374.2578\n",
      "Epoch: 107   Loss: 1689820.2500    Val_Loss: 1245311.6484\n",
      "Epoch: 108   Loss: 1685132.8750    Val_Loss: 1241496.0547\n",
      "Epoch: 109   Loss: 1680701.7500    Val_Loss: 1237916.9766\n",
      "Epoch: 110   Loss: 1676516.1250    Val_Loss: 1234564.0312\n",
      "Epoch: 111   Loss: 1672566.7500    Val_Loss: 1231427.0469\n",
      "Epoch: 112   Loss: 1668842.8750    Val_Loss: 1228496.3750\n",
      "Epoch: 113   Loss: 1665334.8750    Val_Loss: 1225761.8984\n",
      "Epoch: 114   Loss: 1662033.3750    Val_Loss: 1223214.4766\n",
      "Epoch: 115   Loss: 1658929.3750    Val_Loss: 1220844.9062\n",
      "Epoch: 116   Loss: 1656012.7500    Val_Loss: 1218643.7578\n",
      "Epoch: 117   Loss: 1653275.5000    Val_Loss: 1216602.7344\n",
      "Epoch: 118   Loss: 1650709.3750    Val_Loss: 1214713.2578\n",
      "Epoch: 119   Loss: 1648304.7500    Val_Loss: 1212967.0000\n",
      "Epoch: 120   Loss: 1646055.0000    Val_Loss: 1211356.0547\n",
      "Epoch: 121   Loss: 1643951.2500    Val_Loss: 1209872.6328\n",
      "Epoch: 122   Loss: 1641986.2500    Val_Loss: 1208509.2578\n",
      "Epoch: 123   Loss: 1640152.7500    Val_Loss: 1207258.8672\n",
      "Epoch: 124   Loss: 1638443.2500    Val_Loss: 1206114.4297\n",
      "Epoch: 125   Loss: 1636851.1250    Val_Loss: 1205069.1172\n",
      "Epoch: 126   Loss: 1635369.6250    Val_Loss: 1204116.7422\n",
      "Epoch: 127   Loss: 1633992.6250    Val_Loss: 1203250.7891\n",
      "Epoch: 128   Loss: 1632713.7500    Val_Loss: 1202465.7109\n",
      "Epoch: 129   Loss: 1631527.5000    Val_Loss: 1201755.8203\n",
      "Epoch: 130   Loss: 1630427.8750    Val_Loss: 1201115.4922\n",
      "Epoch: 131   Loss: 1629409.3750    Val_Loss: 1200539.9141\n",
      "Epoch: 132   Loss: 1628467.1250    Val_Loss: 1200023.9141\n",
      "Epoch: 133   Loss: 1627596.1250    Val_Loss: 1199562.9609\n",
      "Epoch: 134   Loss: 1626791.7500    Val_Loss: 1199152.6641\n",
      "Epoch: 135   Loss: 1626049.8750    Val_Loss: 1198788.8750\n",
      "Epoch: 136   Loss: 1625365.3750    Val_Loss: 1198467.7188\n",
      "Epoch: 137   Loss: 1624735.2500    Val_Loss: 1198185.4609\n",
      "Epoch: 138   Loss: 1624155.2500    Val_Loss: 1197938.6172\n",
      "Epoch: 139   Loss: 1623621.7500    Val_Loss: 1197724.0781\n",
      "Epoch: 140   Loss: 1623131.6250    Val_Loss: 1197538.5078\n",
      "Epoch: 141   Loss: 1622681.5000    Val_Loss: 1197379.3516\n",
      "Epoch: 142   Loss: 1622268.5000    Val_Loss: 1197243.9297\n",
      "Epoch: 143   Loss: 1621889.7500    Val_Loss: 1197129.7188\n",
      "Epoch: 144   Loss: 1621542.5000    Val_Loss: 1197034.4219\n",
      "Epoch: 145   Loss: 1621224.8750    Val_Loss: 1196956.0625\n",
      "Epoch: 146   Loss: 1620933.7500    Val_Loss: 1196892.5781\n",
      "Epoch: 147   Loss: 1620667.6250    Val_Loss: 1196842.3828\n",
      "Epoch: 148   Loss: 1620424.3750    Val_Loss: 1196803.6406\n",
      "Epoch: 149   Loss: 1620202.0000    Val_Loss: 1196775.0234\n",
      "Epoch: 150   Loss: 1619999.0000    Val_Loss: 1196755.1094\n",
      "Epoch: 151   Loss: 1619813.3750    Val_Loss: 1196742.7188\n",
      "Epoch: 152   Loss: 1619644.0000    Val_Loss: 1196736.7266\n",
      "Epoch: 153   Loss: 1619489.6250    Val_Loss: 1196736.1562\n",
      "Epoch: 154   Loss: 1619348.8750    Val_Loss: 1196740.1797\n",
      "Epoch: 155   Loss: 1619220.5000    Val_Loss: 1196747.8672\n",
      "Epoch: 156   Loss: 1619103.3750    Val_Loss: 1196758.6094\n",
      "Epoch: 157   Loss: 1618996.6250    Val_Loss: 1196771.7656\n",
      "Epoch: 158   Loss: 1618899.6250    Val_Loss: 1196786.9219\n",
      "Epoch: 159   Loss: 1618811.0000    Val_Loss: 1196803.0469\n",
      "Epoch: 160   Loss: 1618730.5000    Val_Loss: 1196820.6328\n",
      "Epoch: 161   Loss: 1618657.2500    Val_Loss: 1196838.6094\n",
      "Epoch: 162   Loss: 1618590.3750    Val_Loss: 1196857.0703\n",
      "Epoch: 163   Loss: 1618520.5000    Val_Loss: 1196893.6797\n",
      "Epoch: 164   Loss: 1618440.6250    Val_Loss: 1196906.3906\n",
      "Epoch: 165   Loss: 1618393.2500    Val_Loss: 1196923.5078\n",
      "Epoch: 166   Loss: 1618351.1250    Val_Loss: 1196940.4609\n",
      "Epoch: 167   Loss: 1618312.6250    Val_Loss: 1196956.6719\n",
      "Epoch: 168   Loss: 1618277.7500    Val_Loss: 1196972.2891\n",
      "Epoch: 169   Loss: 1618246.1250    Val_Loss: 1196987.1953\n",
      "Epoch: 170   Loss: 1618217.5000    Val_Loss: 1197001.1875\n",
      "Epoch: 171   Loss: 1618191.3750    Val_Loss: 1197014.3984\n",
      "Epoch: 172   Loss: 1618167.6250    Val_Loss: 1197026.7812\n",
      "Epoch: 173   Loss: 1618146.3750    Val_Loss: 1197038.4844\n",
      "Epoch: 174   Loss: 1618126.8750    Val_Loss: 1197049.2812\n",
      "Epoch: 175   Loss: 1618109.3750    Val_Loss: 1197059.2969\n",
      "Epoch: 176   Loss: 1618093.8750    Val_Loss: 1197068.5859\n",
      "Epoch: 177   Loss: 1618079.5000    Val_Loss: 1197077.1562\n",
      "Epoch: 178   Loss: 1618066.5000    Val_Loss: 1197085.0547\n",
      "Epoch: 179   Loss: 1618055.1250    Val_Loss: 1197092.2188\n",
      "Epoch: 180   Loss: 1618044.7500    Val_Loss: 1197098.7344\n",
      "Epoch: 181   Loss: 1618035.3750    Val_Loss: 1197104.7734\n",
      "Epoch: 182   Loss: 1618027.0000    Val_Loss: 1197110.2266\n",
      "Epoch: 183   Loss: 1618019.5000    Val_Loss: 1197115.1094\n",
      "Epoch: 184   Loss: 1618012.8750    Val_Loss: 1197119.5625\n",
      "Epoch: 185   Loss: 1618007.1250    Val_Loss: 1197123.5000\n",
      "Epoch: 186   Loss: 1618001.7500    Val_Loss: 1197127.1250\n",
      "Epoch: 187   Loss: 1617996.8750    Val_Loss: 1197130.3516\n",
      "Epoch: 188   Loss: 1617992.6250    Val_Loss: 1197133.2109\n",
      "Epoch: 189   Loss: 1617989.2500    Val_Loss: 1197135.7734\n",
      "Epoch: 190   Loss: 1617985.7500    Val_Loss: 1197138.0859\n",
      "Epoch: 191   Loss: 1617983.0000    Val_Loss: 1197140.1016\n",
      "Epoch: 192   Loss: 1617980.3750    Val_Loss: 1197141.8359\n",
      "Epoch: 193   Loss: 1617978.2500    Val_Loss: 1197143.3906\n",
      "Epoch: 194   Loss: 1617976.3750    Val_Loss: 1197144.8359\n",
      "Epoch: 195   Loss: 1617974.6250    Val_Loss: 1197146.0625\n",
      "Epoch: 196   Loss: 1617973.1250    Val_Loss: 1197147.1250\n",
      "Epoch: 197   Loss: 1617971.7500    Val_Loss: 1197148.0234\n",
      "Epoch: 198   Loss: 1617970.8750    Val_Loss: 1197148.9375\n",
      "Epoch: 199   Loss: 1617969.7500    Val_Loss: 1197149.6797\n",
      "Epoch: 200   Loss: 1617968.8750    Val_Loss: 1197150.3047\n",
      "Epoch: 201   Loss: 1617968.2500    Val_Loss: 1197150.8516\n",
      "Epoch: 202   Loss: 1617967.6250    Val_Loss: 1197151.3203\n",
      "Epoch: 203   Loss: 1617967.0000    Val_Loss: 1197151.8047\n",
      "Epoch: 204   Loss: 1617966.3750    Val_Loss: 1197152.1484\n",
      "Epoch: 205   Loss: 1617965.8750    Val_Loss: 1197152.4375\n",
      "Epoch: 206   Loss: 1617965.7500    Val_Loss: 1197152.7656\n",
      "Epoch: 207   Loss: 1617965.3750    Val_Loss: 1197152.9922\n",
      "Epoch: 208   Loss: 1617965.2500    Val_Loss: 1197153.1562\n",
      "Epoch: 209   Loss: 1617965.0000    Val_Loss: 1197153.3516\n",
      "Epoch: 210   Loss: 1617965.0000    Val_Loss: 1197153.5625\n",
      "Epoch: 211   Loss: 1617964.6250    Val_Loss: 1197153.7109\n",
      "Epoch: 212   Loss: 1617964.3750    Val_Loss: 1197153.7969\n",
      "Epoch: 213   Loss: 1617964.3750    Val_Loss: 1197153.9766\n",
      "Epoch: 214   Loss: 1617964.3750    Val_Loss: 1197154.1016\n",
      "Epoch: 215   Loss: 1617964.3750    Val_Loss: 1197154.1406\n",
      "Epoch: 216   Loss: 1617964.2500    Val_Loss: 1197154.1719\n",
      "Epoch: 217   Loss: 1617964.2500    Val_Loss: 1197154.2734\n",
      "Epoch: 218   Loss: 1617964.2500    Val_Loss: 1197154.4219\n",
      "Epoch: 219   Loss: 1617964.2500    Val_Loss: 1197154.4922\n",
      "Epoch: 220   Loss: 1617888.5000    Val_Loss: 1197215.9219\n",
      "Epoch: 221   Loss: 1617896.2500    Val_Loss: 1197205.5547\n",
      "Epoch: 222   Loss: 1617909.7500    Val_Loss: 1197194.9219\n",
      "Epoch: 223   Loss: 1617921.1250    Val_Loss: 1197186.3203\n",
      "Epoch: 224   Loss: 1617929.8750    Val_Loss: 1197179.6172\n",
      "Epoch: 225   Loss: 1617937.1250    Val_Loss: 1197174.2344\n",
      "Epoch: 226   Loss: 1617942.8750    Val_Loss: 1197170.0234\n",
      "Epoch: 227   Loss: 1617947.3750    Val_Loss: 1197166.6797\n",
      "Epoch: 228   Loss: 1617951.1250    Val_Loss: 1197164.0859\n",
      "Epoch: 229   Loss: 1617954.1250    Val_Loss: 1197162.0469\n",
      "Epoch: 230   Loss: 1617956.2500    Val_Loss: 1197160.5234\n",
      "Epoch: 231   Loss: 1617958.0000    Val_Loss: 1197159.3047\n",
      "Epoch: 232   Loss: 1617959.1250    Val_Loss: 1197158.4219\n",
      "Epoch: 233   Loss: 1617960.1250    Val_Loss: 1197157.6953\n",
      "Epoch: 234   Loss: 1617961.1250    Val_Loss: 1197157.2031\n",
      "Epoch: 235   Loss: 1617961.6250    Val_Loss: 1197156.9062\n",
      "Epoch: 236   Loss: 1617962.2500    Val_Loss: 1197156.6172\n",
      "Epoch: 237   Loss: 1617962.7500    Val_Loss: 1197156.3516\n",
      "Epoch: 238   Loss: 1617962.8750    Val_Loss: 1197156.2266\n",
      "Epoch: 239   Loss: 1617963.0000    Val_Loss: 1197156.1562\n",
      "Epoch: 240   Loss: 1617963.2500    Val_Loss: 1197156.1016\n",
      "Epoch: 241   Loss: 1617963.1250    Val_Loss: 1197156.1094\n",
      "Epoch: 242   Loss: 1617963.2500    Val_Loss: 1197156.0781\n",
      "Epoch: 243   Loss: 1617963.5000    Val_Loss: 1197156.1406\n",
      "Epoch: 244   Loss: 1617963.3750    Val_Loss: 1197156.2812\n",
      "Epoch: 245   Loss: 1617963.3750    Val_Loss: 1197156.2422\n",
      "Epoch: 246   Loss: 1617963.3750    Val_Loss: 1197156.3594\n",
      "Epoch: 247   Loss: 1617963.3750    Val_Loss: 1197156.3828\n",
      "Epoch: 248   Loss: 1617963.3750    Val_Loss: 1197156.4531\n",
      "Epoch: 249   Loss: 1617963.3750    Val_Loss: 1197156.5391\n"
     ]
    }
   ],
   "source": [
    "def train(epochs, model, model_loss):\n",
    "    try: c = model_loss.epoch[-1]\n",
    "    except: c = 0\n",
    "    for epoch in tqdm_notebook(range(epochs),position=0, total = epochs):\n",
    "        losses=[]\n",
    "        dl = iter(xdl)\n",
    "        for t in range(len(dl)):\n",
    "            # Forward pass: compute predicted y and loss by passing x to the model.\n",
    "            xt = next(dl)\n",
    "            y_pred = model(V(xt))\n",
    "            \n",
    "            l = loss(y_pred,V(xt))\n",
    "            losses.append(l)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            l.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        val_dl = iter(vdl)\n",
    "        val_scores = [score(next(val_dl)) for i in range(len(val_dl))]\n",
    "        \n",
    "        model_loss.epoch.append(c+epoch)\n",
    "        model_loss.loss.append(l.item())\n",
    "        model_loss.val_loss.append(np.mean(val_scores))\n",
    "        print(f'Epoch: {epoch}   Loss: {l.item():.4f}    Val_Loss: {np.mean(val_scores):.4f}')\n",
    "\n",
    "epochs=250\n",
    "train(model=model, epochs=epochs, model_loss=model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e7caef4-4a8e-426b-b2ca-3fa0fc90a64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, epochs-1, epochs)\n",
    "\n",
    "plt.plot(x, model_loss.loss, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, model_loss.val_loss, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4882c6b-8559-4fde-ab2d-626086e3e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "p = iter(vdl)\n",
    "preds = np.vstack([model(V(next(p))).cpu().data.numpy() for i in range(len(p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed28b4ca-b48a-404c-8b2b-bcc50ae7209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_nonfraud = np.mean(np.power((valid_x-preds),2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6dc47a50-afcb-4c59-b4bb-91cb04bdacbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7120.0</td>\n",
       "      <td>1.162745e+06</td>\n",
       "      <td>8.496503e+06</td>\n",
       "      <td>20.311716</td>\n",
       "      <td>27971.205471</td>\n",
       "      <td>127819.316348</td>\n",
       "      <td>438996.281626</td>\n",
       "      <td>1.709867e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>802.0</td>\n",
       "      <td>1.494508e+06</td>\n",
       "      <td>1.001448e+07</td>\n",
       "      <td>59.250163</td>\n",
       "      <td>26332.773347</td>\n",
       "      <td>125980.760591</td>\n",
       "      <td>468445.439956</td>\n",
       "      <td>1.697571e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true   count          mean           std        min           25%  \\\n",
       "0     0  7120.0  1.162745e+06  8.496503e+06  20.311716  27971.205471   \n",
       "1     1   802.0  1.494508e+06  1.001448e+07  59.250163  26332.773347   \n",
       "\n",
       "             50%            75%           max  \n",
       "0  127819.316348  438996.281626  1.709867e+08  \n",
       "1  125980.760591  468445.439956  1.697571e+08  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df = pd.DataFrame(data = {'error':error_nonfraud,'true':valid_y})\n",
    "\n",
    "error_df.groupby('true')['error'].describe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f30b2d78-df43-4709-aa45-060be3eea49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7120\n",
       "1     802\n",
       "Name: Abnormal, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.Abnormal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a33ffe0-7a2a-458f-92c0-f59d4e9580a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "val_dl = iter(vdl)\n",
    "val_preds = [get_pred(next(val_dl)) for i in range(len(val_dl))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8233ae3-cf5e-412a-bfd3-7ac66b8b668b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 68.6016, 103.3202,  69.3593,  ..., 128.0390, 138.1360, 129.1872],\n",
       "        [ 68.6016, 103.3202,  69.3593,  ..., 128.0390, 138.1360, 129.1872],\n",
       "        [ 68.6016, 103.3202,  69.3593,  ..., 128.0390, 138.1360, 129.1872],\n",
       "        ...,\n",
       "        [ 68.6016, 103.3202,  69.3593,  ..., 128.0390, 138.1360, 129.1872],\n",
       "        [ 68.6016, 103.3202,  69.3593,  ..., 128.0390, 138.1360, 129.1872],\n",
       "        [ 68.6016, 103.3202,  69.3593,  ..., 128.0390, 138.1360, 129.1872]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d4811-c22e-443a-a04f-e49e6cb72ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05414fb-994e-4be0-98f1-4b1594f4f208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadca82-cd32-435f-a468-08be037a532e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d79ae6-4df4-4e63-bf39-54b710079adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
